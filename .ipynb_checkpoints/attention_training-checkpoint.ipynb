{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention 모델 학습하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()  # tensorflow v1이라면 필요합니다. tensorflow v2라면 주석처리해 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "# from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot_data = pd.read_csv('./all_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SD카드 망가졌어</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SD카드 안돼</td>\n",
       "      <td>다시 새로 사는 게 마음 편해요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SNS 맞팔 왜 안하지ㅠㅠ</td>\n",
       "      <td>잘 모르고 있을 수도 있어요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SNS 시간낭비인 거 아는데 매일 하는 중</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SNS 시간낭비인데 자꾸 보게됨</td>\n",
       "      <td>시간을 정하고 해보세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SNS보면 나만 빼고 다 행복해보여</td>\n",
       "      <td>자랑하는 자리니까요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>가끔 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>가끔 뭐하는지 궁금해</td>\n",
       "      <td>그 사람도 그럴 거예요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>가끔은 혼자인게 좋다</td>\n",
       "      <td>혼자를 즐기세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>가난한 자의 설움</td>\n",
       "      <td>돈은 다시 들어올 거예요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>가만 있어도 땀난다</td>\n",
       "      <td>땀을 식혀주세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>가상화폐 쫄딱 망함</td>\n",
       "      <td>어서 잊고 새출발 하세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>가스불 켜고 나갔어</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>가스불 켜놓고 나온거 같아</td>\n",
       "      <td>빨리 집에 돌아가서 끄고 나오세요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>가스비 너무 많이 나왔다.</td>\n",
       "      <td>다음 달에는 더 절약해봐요.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Q                    A  data\n",
       "0                    12시 땡!           하루가 또 가네요.     2\n",
       "1               1지망 학교 떨어졌어            위로해 드립니다.     2\n",
       "2              3박4일 놀러가고 싶다          여행은 언제나 좋죠.     2\n",
       "3           3박4일 정도 놀러가고 싶다          여행은 언제나 좋죠.     2\n",
       "4                   PPL 심하네           눈살이 찌푸려지죠.     2\n",
       "5                 SD카드 망가졌어   다시 새로 사는 게 마음 편해요.     2\n",
       "6                   SD카드 안돼   다시 새로 사는 게 마음 편해요.     2\n",
       "7            SNS 맞팔 왜 안하지ㅠㅠ     잘 모르고 있을 수도 있어요.     2\n",
       "8   SNS 시간낭비인 거 아는데 매일 하는 중        시간을 정하고 해보세요.     2\n",
       "9         SNS 시간낭비인데 자꾸 보게됨        시간을 정하고 해보세요.     2\n",
       "10      SNS보면 나만 빼고 다 행복해보여          자랑하는 자리니까요.     2\n",
       "11                   가끔 궁금해        그 사람도 그럴 거예요.     2\n",
       "12              가끔 뭐하는지 궁금해        그 사람도 그럴 거예요.     2\n",
       "13              가끔은 혼자인게 좋다            혼자를 즐기세요.     2\n",
       "14                가난한 자의 설움       돈은 다시 들어올 거예요.     2\n",
       "15               가만 있어도 땀난다            땀을 식혀주세요.     2\n",
       "16               가상화폐 쫄딱 망함       어서 잊고 새출발 하세요.     2\n",
       "17               가스불 켜고 나갔어  빨리 집에 돌아가서 끄고 나오세요.     2\n",
       "18           가스불 켜놓고 나온거 같아  빨리 집에 돌아가서 끄고 나오세요.     2\n",
       "19           가스비 너무 많이 나왔다.      다음 달에는 더 절약해봐요.     2"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chatbot_data = chatbot_data[:20]  # 테스트를 위해 20개의 데이터를 사용해 보았습니다.\n",
    "chatbot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = list(chatbot_data.Q)\n",
    "answer= list(chatbot_data.A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 태그 단어\n",
    "PAD = \"<PADDING>\"   # 패딩\n",
    "STA = \"<START>\"     # 시작\n",
    "END = \"<END>\"       # 끝\n",
    "OOV = \"<OOV>\"       # 없는 단어(Out of Vocabulary)\n",
    "\n",
    "# 태그 인덱스\n",
    "PAD_INDEX = 0\n",
    "STA_INDEX = 1\n",
    "END_INDEX = 2\n",
    "OOV_INDEX = 3\n",
    "\n",
    "# 데이터 타입\n",
    "ENCODER_INPUT  = 0\n",
    "DECODER_INPUT  = 1\n",
    "DECODER_TARGET = 2\n",
    "\n",
    "# 한 문장에서 단어 시퀀스의 최대 개수\n",
    "max_sequences = 20\n",
    "\n",
    "# 임베딩 벡터 차원\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기 Mecab 사용하기\n",
    "형태소 분석기로 Mecab을 사용하였습니다.\n",
    "\n",
    "Mecab은 별도로 다운로드 해주셔야 사용할 수 있습니다.\n",
    "(https://cleancode-ws.tistory.com/97 를 참고해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCab 형태소 분석기 사용하기\n",
    "# 토크화를 위해서 함수를 정의해 줍니다. \n",
    "\n",
    "import MeCab\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "def mecab_morphs(text):\n",
    "    morphs = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS의 형태를 추출하는 정규표현식.\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정.\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    # 추출한 token중에 문자열만 선택.\n",
    "    for token in temp:\n",
    "        morphs.append(token[0])\n",
    "    \n",
    "    return morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수기호 제거 및 토큰화 함수\n",
    "\n",
    "def pos_tag_korea(sentences):\n",
    "    # KoNLPy 형태소분석기 설정\n",
    "    # 문장 품사 변수 초기화\n",
    "    sentences_pos = []\n",
    "    # 모든 문장 반복\n",
    "    for sentence in sentences:\n",
    "        # 특수기호 제거\n",
    "        RE_FILTER = re.compile(\"[!#$%&()*+,-/:;<=>@[\\\\]^_`{|}~]\")\n",
    "        sentence = re.sub(RE_FILTER, \"\", sentence)     \n",
    "        # 배열인 형태소분석의 출력을 띄어쓰기로 구분하여 붙임\n",
    "        sentence = \" \".join(mecab_morphs(sentence))\n",
    "        sentences_pos.append(sentence)\n",
    "    return sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q, A 데이터 토큰화\n",
    "mecab_question = pos_tag_korea(question)\n",
    "mecab_answer = pos_tag_korea(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 사전 만들기\n",
    "학습 데이터로 단어 사전을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 단어 사전 만들기\n",
    "# 질문과 대답 문장들을 하나로 합침\n",
    "sentences = []\n",
    "sentences.extend(mecab_question)\n",
    "sentences.extend(mecab_answer)\n",
    "\n",
    "words = []\n",
    "\n",
    "# 단어들의 배열 생성\n",
    "for sentence in sentences:\n",
    "    for word in sentence.split():\n",
    "        words.append(word)\n",
    "\n",
    "# 길이가 0인 단어는 삭제\n",
    "words = [word for word in words if len(word) > 0]\n",
    "\n",
    "# 중복된 단어 삭제\n",
    "words = list(set(words))\n",
    "\n",
    "# 제일 앞에 태그 단어 삽입\n",
    "words[:0] = [PAD, STA, END, OOV]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 사전 개수\n",
    "vocab_size = len(words) \n",
    "len(words)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어와 인덱스의 딕셔너리 생성\n",
    "word_to_index = {word: index for index, word in enumerate(words)}\n",
    "index_to_word = {index: word for index, word in enumerate(words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장을 index로 변경\n",
    "토큰화된 문장을 index형태로 변경해 줍니다.\n",
    "\n",
    "문장 리스트 길이를 모두 일치시키기 위해, padding을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 문장 속 형태소(단어)를 인덱스로 변경하는 함수 작성\n",
    "# max_sequences에 맞춰서 padding을 줌\n",
    "# 디코더 input 데이터: <start> 태그 추가\n",
    "# 디코더 oupt 데이터: <end> 태그 추가\n",
    "\n",
    "def convert_text_to_index(sentences, vocabulary, type): \n",
    "    \n",
    "    sentences_index = []\n",
    "    \n",
    "    # 모든 문장에 대해서 반복\n",
    "    for sentence in sentences:\n",
    "        sentence_index = []\n",
    "\n",
    "        # 디코더 입력일 경우 맨 앞에 START 태그 추가\n",
    "        if type == DECODER_INPUT:\n",
    "            sentence_index.extend([vocabulary[STA]])\n",
    "        \n",
    "        # 문장의 단어들을 띄어쓰기로 분리\n",
    "        for word in sentence.split():\n",
    "            if vocabulary.get(word) is not None:\n",
    "                # 사전에 있는 단어면 해당 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[word]])\n",
    "            else:\n",
    "                # 사전에 없는 단어면 OOV 인덱스를 추가\n",
    "                sentence_index.extend([vocabulary[OOV]])\n",
    "\n",
    "        # 최대 길이 검사\n",
    "        if type == DECODER_TARGET:\n",
    "            # 디코더 목표일 경우 맨 뒤에 END 태그 추가\n",
    "            if len(sentence_index) >= max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences-1] + [vocabulary[END]]\n",
    "            else:\n",
    "                sentence_index += [vocabulary[END]]\n",
    "        else:\n",
    "            if len(sentence_index) > max_sequences:\n",
    "                sentence_index = sentence_index[:max_sequences]\n",
    "            \n",
    "        # 최대 길이에 없는 공간은 패딩 인덱스로 채움\n",
    "        sentence_index += (max_sequences - len(sentence_index)) * [vocabulary[PAD]]\n",
    "        \n",
    "        # 문장의 인덱스 배열을 추가\n",
    "        sentences_index.append(sentence_index)\n",
    "\n",
    "    return np.asarray(sentences_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12 시 땡'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_question[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([140, 143,  98,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인코더 입력 인덱스 변환\n",
    "x_encoder = convert_text_to_index(mecab_question, word_to_index, ENCODER_INPUT)\n",
    "\n",
    "# 첫 번째 인코더 입력 출력\n",
    "x_encoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'하루 가 또 가 네요'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab_answer[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,  14,  46,  36,  46, 124,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 입력 인덱스 변환\n",
    "x_decoder = convert_text_to_index(mecab_answer, word_to_index, DECODER_INPUT)\n",
    "\n",
    "# 첫 번째 디코더 입력 출력 (START 하루 가 또 가네요)\n",
    "x_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14,  46,  36,  46, 124,   2,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디코더 목표 인덱스 변환\n",
    "y_decoder = convert_text_to_index(mecab_answer, word_to_index, DECODER_TARGET)\n",
    "\n",
    "# 첫 번째 디코더 목표 출력 (하루 가 또 가네요 END)\n",
    "y_decoder[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실함수 sparse_categorical_crossentropy로 사용하기 위해, \n",
    "y_decoder = y_decoder.reshape(-1,max_sequences,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습시켜놓은 word2vec 가져오기\n",
    "\n",
    "keras Embedding 층의 초기값으로 word2vec 벡터를 줄 것입니다.\n",
    "\n",
    "그러나 이 임베딩값은 고정되지 않고, 학습시에 데이터에 적절하게 변경됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359672, 200)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# 학습시킨 word2vec 가져오기\n",
    "ko_model = gensim.models.Word2Vec.load('./word2vec/word2vec_model_200')\n",
    "ko_model.wv.vectors.shape #30만 단어 200차원으로 학습한 모델링 가지고 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 200)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding 초기값\n",
    "embedding_matrix = np.zeros((vocab_size, 200))\n",
    "# 단어 집합 크기의 행과 200개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in ko_model:\n",
    "        return ko_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <PADDING>\n",
      "1 <START>\n",
      "2 <END>\n",
      "3 <OOV>\n",
      "4 주\n",
      "5 다\n",
      "6 뭐\n",
      "7 여행\n",
      "8 카드\n",
      "9 자리\n",
      "10 맞\n",
      "11 싶\n",
      "12 혼자\n",
      "13 SNS\n",
      "14 하루\n",
      "15 어\n",
      "16 들어올\n",
      "17 하\n",
      "18 새로\n",
      "19 도\n",
      "20 고\n",
      "21 있\n",
      "22 낭비\n",
      "23 편해요\n",
      "24 놓\n",
      "25 PPL\n",
      "26 러\n",
      "27 안\n",
      "28 언제나\n",
      "29 어요\n",
      "30 나오\n",
      "31 거\n",
      "32 예요\n",
      "33 다음\n",
      "34 중\n",
      "35 정하\n",
      "36 또\n",
      "37 화폐\n",
      "38 면\n",
      "39 모르\n",
      "40 수\n",
      "41 에\n",
      "42 사람\n",
      "43 왜\n",
      "44 어서\n",
      "45 팔\n",
      "46 가\n",
      "47 나갔\n",
      "48 게\n",
      "49 시간\n",
      "50 박\n",
      "51 끄\n",
      "52 돼\n",
      "53 잘\n",
      "54 지망\n",
      "55 을\n",
      "56 놀\n",
      "57 니까요\n",
      "58 매일\n",
      "59 지\n",
      "60 사\n",
      "61 좋\n",
      "62 빼\n",
      "63 쫄딱\n",
      "64 새\n",
      "65 세요\n",
      "66 보여\n",
      "67 은\n",
      "68 즐기\n",
      "69 인데\n",
      "70 이\n",
      "71 다시\n",
      "72 서\n",
      "73 난다\n",
      "74 SD\n",
      "75 보\n",
      "76 출발\n",
      "77 돌아가\n",
      "78 해\n",
      "79 빨리\n",
      "80 그\n",
      "81 일\n",
      "82 네\n",
      "83 집\n",
      "84 나왔\n",
      "85 불\n",
      "86 비\n",
      "87 식혀\n",
      "88 나\n",
      "89 자랑\n",
      "90 설움\n",
      "91 켜\n",
      "92 마음\n",
      "93 가스\n",
      "94 는지\n",
      "95 더\n",
      "96 그럴\n",
      "97 위로\n",
      "98 땡\n",
      "99 자\n",
      "100 많이\n",
      "101 가끔\n",
      "102 망함\n",
      "103 망가졌\n",
      "104 한\n",
      "105 죠\n",
      "106 눈살\n",
      "107 절약\n",
      "108 아\n",
      "109 행복\n",
      "110 가난\n",
      "111 찌푸려\n",
      "112 인\n",
      "113 자꾸\n",
      "114 1\n",
      "115 어도\n",
      "116 됨\n",
      "117 떨어졌\n",
      "118 돈\n",
      "119 궁금\n",
      "120 3\n",
      "121 드립니다\n",
      "122 의\n",
      "123 를\n",
      "124 네요\n",
      "125 같\n",
      "126 잊\n",
      "127 는\n",
      "128 는데\n",
      "129 너무\n",
      "130 땀\n",
      "131 가만\n",
      "132 나온\n",
      "133 학교\n",
      "134 4\n",
      "135 심하\n",
      "136 해봐요\n",
      "137 ㅠㅠ\n",
      "138 정도\n",
      "139 가상\n",
      "140 12\n",
      "141 만\n",
      "142 달\n",
      "143 시\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heeju\\Anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\heeju\\Anaconda3\\envs\\tf1\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate(words):\n",
    "    print(idx, val)\n",
    "    temp = get_vector(val) # 단어(key) 해당되는 임베딩 벡터의 200개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
    "        embedding_matrix[idx] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-1.9500165 , -0.35533634, -0.7207588 , ..., -0.69542342,\n",
       "         0.1870188 ,  4.07732391],\n",
       "       [-0.72195613,  2.04154825, -1.73331285, ..., -0.7251206 ,\n",
       "        -0.8872419 ,  1.98916161],\n",
       "       [-2.04303956, -1.8580097 ,  2.12866902, ...,  2.63733053,\n",
       "        -0.89894992,  3.83264709]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# 훈련 모델 인코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "encoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "# 임베딩 레이어  Embedding(단어수, 임베딩차원)\n",
    "encoder_outputs = layers.Embedding(len(words), embedding_dim, weights=[embedding_matrix], name='en-embedding')(encoder_inputs)\n",
    "\n",
    "# return_state가 True면 상태값 리턴\n",
    "# LSTM은 state_h(hidden state)와 state_c(cell state) 2개의 상태 존재\n",
    "encoder_outputs, state_h, state_c = layers.LSTM(128,\n",
    "                                                dropout=0.1,\n",
    "                                                recurrent_dropout=0.5,\n",
    "                                                kernel_initializer = 'he_normal',\n",
    "                                                name='en-lstm',\n",
    "                                                return_state=True,\n",
    "                                                return_sequences=True)(encoder_outputs)\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "# encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# 훈련 모델 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
    "decoder_inputs = layers.Input(shape=(None,))\n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_embedding = layers.Embedding(len(words), embedding_dim, weights=[embedding_matrix], name='de_embedding')\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "# 인코더와 달리 return_sequences를 True로 설정하여 모든 타임 스텝 출력값 리턴\n",
    "# 모든 타임 스텝의 출력값들을 다음 레이어의 Dense()로 처리하기 위함\n",
    "decoder_lstm = layers.LSTM(128,\n",
    "                           dropout=0.1,\n",
    "                           recurrent_dropout=0.5,\n",
    "                           kernel_initializer = 'he_normal',\n",
    "                           name='de-lstm',\n",
    "                           return_state=True,\n",
    "                           return_sequences=True)\n",
    "\n",
    "# initial_state를 인코더의 상태로 초기화\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_outputs,\n",
    "                                     initial_state=[state_h,state_c])\n",
    "# 어텐션 추가\n",
    "Q = tf.concat([state_h[:, tf.newaxis, :], decoder_outputs[:, :-1, :]], axis=1)\n",
    "att_value = layers.Attention()([Q , encoder_outputs])\n",
    "y = tf.concat([decoder_outputs, att_value], axis=-1)\n",
    "\n",
    "\n",
    "\n",
    "# 단어의 개수만큼 노드의 개수를 설정하여 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_dense = layers.Dense(len(words), activation='softmax',kernel_initializer = 'he_normal')\n",
    "decoder_outputs = decoder_dense(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# 훈련 모델 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 입력과 출력으로 함수형 API 모델 생성\n",
    "model = models.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# 학습 방법 설정\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#  예측 모델 인코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 훈련 모델의 인코더 상태를 사용하여 예측 모델 인코더 설정\n",
    "encoder_model = models.Model(encoder_inputs, [encoder_outputs, state_h, state_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, None)]            0         \n",
      "_________________________________________________________________\n",
      "en-embedding (Embedding)     (None, None, 200)         28800     \n",
      "_________________________________________________________________\n",
      "en-lstm (LSTM)               [(None, None, 128), (None 168448    \n",
      "=================================================================\n",
      "Total params: 197,248\n",
      "Trainable params: 197,248\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# 예측 모델 디코더 정의\n",
    "#--------------------------------------------\n",
    "\n",
    "# 예측시에는 훈련시와 달리 타임 스텝을 한 단계씩 수행\n",
    "# 매번 이전 디코더 상태를 입력으로 받아서 새로 설정\n",
    "decoder_state_input_h = layers.Input(shape=(128,))\n",
    "decoder_state_input_c = layers.Input(shape=(128,))\n",
    "H = layers.Input(shape=(max_sequences,128))\n",
    "# decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]    \n",
    "\n",
    "# 임베딩 레이어\n",
    "decoder_outputs = decoder_embedding(decoder_inputs)\n",
    "\n",
    "# LSTM 레이어\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_outputs,\n",
    "                                                 initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "# 히든 상태와 셀 상태를 하나로 묶음\n",
    "# decoder_states = [state_h, state_c]\n",
    "\n",
    "# 어텐션 추가\n",
    "Q = tf.concat([decoder_state_input_h[:, tf.newaxis, :], decoder_outputs[:, :-1, :]], axis=1)  \n",
    "att_value = layers.Attention(name='att_layer')([Q ,H])\n",
    "y = tf.concat([decoder_outputs, att_value], axis=-1)\n",
    "\n",
    "attention_weights = tf.matmul(Q, H, transpose_b=True)\n",
    "attention_weights = tf.nn.softmax(attention_weights)\n",
    "\n",
    "# Dense 레이어를 통해 원핫 형식으로 각 단어 인덱스를 출력\n",
    "decoder_outputs = decoder_dense(y)\n",
    "\n",
    "# 예측 모델 디코더 설정\n",
    "decoder_model = models.Model([decoder_inputs] + [H,decoder_state_input_h, decoder_state_input_c], [decoder_outputs] + [state_h, state_c, attention_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "de_embedding (Embedding)        (None, None, 200)    28800       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "de-lstm (LSTM)                  [(None, None, 128),  168448      de_embedding[1][0]               \n",
      "                                                                 input_8[0][0]                    \n",
      "                                                                 input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_6 (Te [(None, 1, 128)]     0           input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice_7 (Te [(None, None, 128)]  0           de-lstm[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_6 (TensorFlo [(None, None, 128)]  0           tf_op_layer_strided_slice_6[0][0]\n",
      "                                                                 tf_op_layer_strided_slice_7[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           [(None, 20, 128)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "att_layer (Attention)           (None, None, 128)    0           tf_op_layer_concat_6[0][0]       \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_7 (TensorFlo [(None, None, 256)]  0           de-lstm[1][0]                    \n",
      "                                                                 att_layer[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_1 (TensorFlo [(None, None, 20)]   0           tf_op_layer_concat_6[0][0]       \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 144)    37008       tf_op_layer_concat_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_1 (TensorFl [(None, None, 20)]   0           tf_op_layer_MatMul_1[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 234,256\n",
      "Trainable params: 234,256\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인덱스를 문장으로 변환\n",
    "def convert_index_to_text(indexs, vocabulary): \n",
    "    sentence = ''\n",
    "    # 모든 문장에 대해서 반복\n",
    "    for index in indexs:\n",
    "        if index == END_INDEX:\n",
    "            # 종료 인덱스면 중지\n",
    "            break;\n",
    "        if vocabulary.get(index) is not None:\n",
    "            # 사전에 있는 인덱스면 해당 단어를 추가\n",
    "            sentence += vocabulary[index]\n",
    "        else:\n",
    "            # 사전에 없는 인덱스면 OOV 단어를 추가\n",
    "            sentence.extend([vocabulary[OOV_INDEX]])\n",
    "\n",
    "        # 빈칸 추가\n",
    "        sentence += ' '\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Epoch : 1\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 11s 573ms/sample - loss: 5.0010 - acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 4.4340 - acc: 0.6275\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 3.8344 - acc: 0.6450\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 2.9895 - acc: 0.6400\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 2.1330 - acc: 0.6400\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 1.7578 - acc: 0.6400\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 9ms/sample - loss: 1.5850 - acc: 0.6450\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 1.5009 - acc: 0.6425\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 1.3830 - acc: 0.6650\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 9ms/sample - loss: 1.3056 - acc: 0.6650\n",
      "accuracy : 0.665\n",
      "loss : 1.3056252002716064\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "<PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> <PADDING> \n",
      "\n",
      "Total Epoch : 2\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 1.2331 - acc: 0.6925\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 1.1755 - acc: 0.7225\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 11ms/sample - loss: 1.1133 - acc: 0.7325\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 1.0536 - acc: 0.7525\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 1.0073 - acc: 0.7550\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 13ms/sample - loss: 0.9668 - acc: 0.7800\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.9111 - acc: 0.8050\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.8620 - acc: 0.8150\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.8195 - acc: 0.8450\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.7937 - acc: 0.8675\n",
      "accuracy : 0.8675\n",
      "loss : 0.7936820983886719\n",
      "[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "<PADDING> <PADDING> <PADDING> \n",
      "\n",
      "Total Epoch : 3\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.7424 - acc: 0.8775\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.6920 - acc: 0.8900\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.6665 - acc: 0.9025\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.6359 - acc: 0.9100\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.5952 - acc: 0.9200\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 12ms/sample - loss: 0.5807 - acc: 0.9175\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 11ms/sample - loss: 0.5324 - acc: 0.9275\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 9ms/sample - loss: 0.5034 - acc: 0.9525\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.4842 - acc: 0.9475\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.4694 - acc: 0.9475\n",
      "accuracy : 0.9475\n",
      "loss : 0.46936553716659546\n",
      "[ 49  46  46  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "시간 가 가 가 네요 \n",
      "\n",
      "Total Epoch : 4\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 9ms/sample - loss: 0.4430 - acc: 0.9450\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.4147 - acc: 0.9550\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.3979 - acc: 0.9575\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.3710 - acc: 0.9600\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.3583 - acc: 0.9625\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.3491 - acc: 0.9675\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.3243 - acc: 0.9625\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.3076 - acc: 0.9650\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.2978 - acc: 0.9700\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.2868 - acc: 0.9700\n",
      "accuracy : 0.97\n",
      "loss : 0.28684911131858826\n",
      "[ 49  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "시간 가 또 가 네요 \n",
      "\n",
      "Total Epoch : 5\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.2701 - acc: 0.9700\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.2629 - acc: 0.9675\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.2435 - acc: 0.9700\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.2299 - acc: 0.9750\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 9ms/sample - loss: 0.2281 - acc: 0.9725\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 11ms/sample - loss: 0.2158 - acc: 0.9700\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.2047 - acc: 0.9750\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1998 - acc: 0.9775\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1927 - acc: 0.9750\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.1856 - acc: 0.9750\n",
      "accuracy : 0.975\n",
      "loss : 0.1855614334344864\n",
      "[ 49  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "시간 가 또 가 네요 \n",
      "\n",
      "Total Epoch : 6\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1724 - acc: 0.9800\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.1704 - acc: 0.9775\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1593 - acc: 0.9850\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1502 - acc: 0.9775\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1533 - acc: 0.9850\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1417 - acc: 0.9825\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.1402 - acc: 0.9825\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1303 - acc: 0.9800\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1241 - acc: 0.9875\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.1209 - acc: 0.9850\n",
      "accuracy : 0.985\n",
      "loss : 0.12093163281679153\n",
      "[ 14  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "하루 가 또 가 네요 \n",
      "\n",
      "Total Epoch : 7\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1185 - acc: 0.9850\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1122 - acc: 0.9875\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.1084 - acc: 0.9900\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0999 - acc: 0.9900\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 9ms/sample - loss: 0.0996 - acc: 0.9875\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0898 - acc: 0.9950\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0901 - acc: 0.9925\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0887 - acc: 0.9900\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0794 - acc: 0.9950\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0811 - acc: 0.9950\n",
      "accuracy : 0.995\n",
      "loss : 0.0810946449637413\n",
      "[ 14  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "하루 가 또 가 네요 \n",
      "\n",
      "Total Epoch : 8\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0738 - acc: 0.9900\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.0684 - acc: 0.9950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.0703 - acc: 0.9950\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0641 - acc: 0.9975\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0620 - acc: 0.9950\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0600 - acc: 0.9875\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.0622 - acc: 0.9925\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0580 - acc: 0.9925\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0535 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0522 - acc: 0.9950\n",
      "accuracy : 0.995\n",
      "loss : 0.05219646543264389\n",
      "[ 14  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "하루 가 또 가 네요 \n",
      "\n",
      "Total Epoch : 9\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0457 - acc: 1.0000\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0384 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0441 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0440 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 10ms/sample - loss: 0.0428 - acc: 0.9975\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 8ms/sample - loss: 0.0407 - acc: 0.9975\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0345 - acc: 0.9975\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0329 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0285 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0283 - acc: 1.0000\n",
      "accuracy : 1.0\n",
      "loss : 0.02826610393822193\n",
      "[ 14  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "하루 가 또 가 네요 \n",
      "\n",
      "Total Epoch : 10\n",
      "Train on 20 samples\n",
      "Epoch 1/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0298 - acc: 1.0000\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0257 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0276 - acc: 0.9975\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0246 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0237 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 0s 7ms/sample - loss: 0.0225 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0207 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0180 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0191 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 0s 6ms/sample - loss: 0.0156 - acc: 1.0000\n",
      "accuracy : 1.0\n",
      "loss : 0.015556217171251774\n",
      "[ 14  46  36  46 124   2   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "하루 가 또 가 네요 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 에폭 반복 #10번 \n",
    "for epoch in range(10):\n",
    "    print('Total Epoch :', epoch + 1)\n",
    "\n",
    "    # 훈련 시작\n",
    "    history = model.fit([x_encoder, x_decoder],\n",
    "                        y_decoder,\n",
    "                        epochs=10,\n",
    "                        batch_size=64,\n",
    "                        verbose=1)\n",
    "    \n",
    "    # 정확도와 손실 출력\n",
    "    print('accuracy :', history.history['acc'][-1])\n",
    "    print('loss :', history.history['loss'][-1])\n",
    "    \n",
    "    # 문장 예측 테스트\n",
    "    # ('톰 은 요령 피우지 않는다') -> (\"Tom doesn't cut any corners.\")\n",
    "    input_encoder = x_encoder[0].reshape(1, x_encoder[0].shape[0])\n",
    "    input_decoder = x_decoder[0].reshape(1, x_decoder[0].shape[0])\n",
    "    results = model.predict([input_encoder, input_decoder])\n",
    "    \n",
    "    # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "    # 1축을 기준으로 가장 높은 값의 위치를 구함\n",
    "    indexs = np.argmax(results[0], 1) \n",
    "    print(indexs)\n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "    print(sentence)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습모델로 문장 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측을 위한 입력 생성 데이터(한글을 형태소 분석 한 뒤, 벡터 배열로 변경하는 함수)\n",
    "def make_predict_input(sentence):\n",
    "\n",
    "    sentences = []\n",
    "    sentences.append(sentence)\n",
    "    sentences = pos_tag_korea(sentences)\n",
    "    input_seq = convert_text_to_index(sentences, word_to_index, 0)\n",
    "    \n",
    "    return input_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성\n",
    "# attention 시각화가 필요없다면 이 함수를 사용하시면 됩니다.\n",
    "\n",
    "def generate_text(input_seq):\n",
    "    ## 입력을 인코더에 넣어 마지막 상태 구함\n",
    "    # states = encoder_model.predict(input_seq)\n",
    "\n",
    "    # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "    H, h, c = encoder_model.predict(input_seq)\n",
    "    states=[h,c]\n",
    "    # 목표 시퀀스 초기화\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "    target_seq[0, 0] = STA_INDEX\n",
    "    \n",
    "    # 인덱스 초기화\n",
    "    indexs = []\n",
    "    \n",
    "    # 디코더 타임 스텝 반복\n",
    "    while 1:\n",
    "        # 디코더로 현재 타임 스텝 출력 구함\n",
    "        # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "        decoder_outputs, h, c, _ = decoder_model.predict(\n",
    "                                                [target_seq] + [H,h,c])\n",
    "\n",
    "        # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "        index = np.argmax(decoder_outputs[0, 0, :])\n",
    "        indexs.append(index)\n",
    "        \n",
    "        # 종료 검사\n",
    "        if index == END_INDEX or len(indexs) >= max_sequences:\n",
    "            break\n",
    "\n",
    "        # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = index\n",
    "        \n",
    "        # 디코더의 이전 상태를 다음 디코더 예측에 사용\n",
    "        # states = [state_h, state_c]\n",
    "\n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "    # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 20}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90, fontproperties=fontprop)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict, fontproperties=fontprop)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여행 은 언제나 좋 죠 \n"
     ]
    }
   ],
   "source": [
    "# 학습모델 예측하기\n",
    "sentence='3박4일 놀러가고 싶다'\n",
    "\n",
    "input_seq = make_predict_input(sentence)\n",
    "translator=generate_text(input_seq) \n",
    "print(translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## attention 가중치값 시각화 하기\n",
    "\n",
    "예측한 문장에 대해서 attention 가중치를 확인하고 싶다면 아래의 코드를 참고하세요.\n",
    "\n",
    "시각화 그래프에 한글을 사용하려면, 한글 글꼴을 다운받아야 합니다.\n",
    "\n",
    "(http://corazzon.github.io/matplotlib_font_setting 여기를 참고하여 다운받아줍니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.font_manager as fm\n",
    "font_path = 'C:/Windows/Fonts/NanumBarunGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path, size=18) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 생성\n",
    "def evaluate(input_sentence):\n",
    "    attention_plot = np.zeros((max_sequences, max_sequences))\n",
    "    \n",
    "    sentences = []\n",
    "    sentences.append(input_sentence)\n",
    "    sentences = pos_tag_korea(sentences)\n",
    "    input_sentence = sentences[0]\n",
    "    input_seq = convert_text_to_index(sentences, word_to_index,ENCODER_INPUT)\n",
    "\n",
    "    # 입력을 인코더에 넣어 마지막 상태 구함\n",
    "    H, h, c = encoder_model.predict(input_seq)\n",
    "    states=[h,c]\n",
    "    # 목표 시퀀스 초기화\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    \n",
    "    # 목표 시퀀스의 첫 번째에 <START> 태그 추가\n",
    "    target_seq[0, 0] = STA_INDEX\n",
    "    \n",
    "    # 인덱스 초기화\n",
    "    indexs = []\n",
    "    \n",
    "    # 디코더 타임 스텝 반복\n",
    "    i = 0\n",
    "    while 1:\n",
    "        # 디코더로 현재 타임 스텝 출력 구함\n",
    "        # 처음에는 인코더 상태를, 다음부터 이전 디코더 상태로 초기화\n",
    "        decoder_outputs, h, c, att_weights = decoder_model.predict(\n",
    "                                                [target_seq] + [H,h,c])\n",
    "\n",
    "        # 결과의 원핫인코딩 형식을 인덱스로 변환\n",
    "        index = np.argmax(decoder_outputs[0, 0, :])\n",
    "        indexs.append(index)\n",
    "        \n",
    "        # 종료 검사\n",
    "        if index == END_INDEX or len(indexs) >= max_sequences:\n",
    "            break\n",
    "\n",
    "        # 목표 시퀀스를 바로 이전의 출력으로 설정\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = index\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(att_weights, (-1, ))\n",
    "        attention_plot[i] = attention_weights.numpy()\n",
    "        i += 1\n",
    "        \n",
    "    # 인덱스를 문장으로 변환\n",
    "    sentence = convert_index_to_text(indexs, index_to_word)\n",
    "    \n",
    "    return sentence, input_sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "  fig = plt.figure(figsize=(10,10))\n",
    "  ax = fig.add_subplot(1, 1, 1)\n",
    "  ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "  fontdict = {'fontsize': 20}\n",
    "\n",
    "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90, fontproperties=fontprop)\n",
    "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict, fontproperties=fontprop)\n",
    "\n",
    "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "  print('Input: %s' % (sentence))\n",
    "  print('Predicted translation: {}'.format(result))\n",
    "  result = result.strip()\n",
    "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 3 박 4 일 놀 러 가 고 싶 다\n",
      "Predicted translation: 여행 은 언제나 좋 죠 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "translate('3박4일 놀러가고 싶다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 심심 하 다\n",
      "Predicted translation: 저 랑 놀 아요 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence='심심하다'\n",
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# 단어사전 저장\n",
    "with open('attention_word_to_index.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_to_index, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('attention_index_to_word.pickle', 'wb') as handle:\n",
    "    pickle.dump(index_to_word, handle, protocol=pickle.HIGHEST_PROTOCOL)    \n",
    "\n",
    "# 인코더, 디코더 모델 저장\n",
    "with open('attention_encoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(encoder_model.to_json())\n",
    "encoder_model.save_weights('attention_encoder_model_weights.h5')\n",
    "\n",
    "with open('attention_decoder_model.json', 'w', encoding='utf8') as f:\n",
    "    f.write(decoder_model.to_json())\n",
    "decoder_model.save_weights('attention_decoder_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1]",
   "language": "python",
   "name": "conda-env-tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
