{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "j9giBYyIuRl7",
    "outputId": "093297a0-d0e9-4a25-cabd-291d48eca0a8"
   },
   "outputs": [],
   "source": [
    "# #구글 드라이버를 마운트\n",
    "# from google.colab import drive\n",
    "\n",
    "# drive.mount('gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\USER\\\\Desktop\\\\KOR\\\\200605_중간점검'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fo-wamRdvy45",
    "outputId": "ad946509-4b17-450c-8124-382ec11ef847"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29005"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "lines= pd.read_csv('../all_data.csv')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "colab_type": "code",
    "id": "KZRK_zwKvyxz",
    "outputId": "3ac64d9a-932a-45c8-8b9f-01eb6c939cec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['12시 땡!', '하루가 또 가네요.'],\n",
       "       ['1지망 학교 떨어졌어', '위로해 드립니다.'],\n",
       "       ['3박4일 놀러가고 싶다', '여행은 언제나 좋죠.'],\n",
       "       ...,\n",
       "       ['한국에 돌아오고 나서 증상이 악화된 거 같아.', '너무 심하시면 병원을 다시 가보는 건 어떨까요?'],\n",
       "       ['약을 끊었더니 증상이 예전보다 더 심해진 것 같아.', '너무 심하시면 병원을 다시 가보는 건 어떨까요?'],\n",
       "       ['통증이 점점 심해지는것 같아.', '너무 심하시면 병원을 다시 가보는 건 어떨까요?']], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines.drop('data',axis=1)\n",
    "lines = lines.values\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "qVcFK64e90tW",
    "outputId": "b1aecda5-fc0f-4863-ec94-3a35d40c887d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\n",
      "ERROR: No matching distribution found for tf-nightly-2.0-preview\n"
     ]
    }
   ],
   "source": [
    "# We will be using TensorFlow 2.0 for this tutorial!\n",
    "!pip install tf-nightly-2.0-preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mE-aLQkh-GR8",
    "outputId": "b62ad4fb-d8f0-4592-e04c-c8fd71ebd0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have version 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"You have version\", tf.__version__)\n",
    "assert tf.__version__ >= \"2.0\" # TensorFlow ≥ 2.0 required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJsNN6j5-d3C"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unicodedata, re\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Lambda, Layer, Embedding, LayerNormalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrMMHJ5TR9lM"
   },
   "source": [
    "# 데이터 전처리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석기 Mecab 사용하기\n",
    "형태소 분석기로 Mecab을 사용하였습니다.\n",
    "\n",
    "Mecab은 별도로 다운로드 해주셔야 사용할 수 있습니다.\n",
    "(https://cleancode-ws.tistory.com/97 를 참고해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1AdHWzm5rlG"
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import re\n",
    "\n",
    "mecab = MeCab.Tagger()\n",
    "\n",
    "def mecab_morphs(text):\n",
    "    morphs = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS의 형태를 추출하는 정규표현식.\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정.\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    # 추출한 token중에 문자열만 선택.\n",
    "    for token in temp:\n",
    "        morphs.append(token[0])\n",
    "\n",
    "    return morphs\n",
    "\n",
    "def mecab_tagging(text):\n",
    "    morphs = []\n",
    "    \n",
    "    # 우리가 원하는 TOKEN\\tPOS의 형태를 추출하는 정규표현식.\n",
    "    pattern = re.compile(\".*\\t[A-Z]+\") \n",
    "    \n",
    "    # 패턴에 맞는 문자열을 추출하여 konlpy의 mecab 결과와 같아지도록 수정.\n",
    "    temp = [tuple(pattern.match(token).group(0).split(\"\\t\")) for token in mecab.parse(text).splitlines()[:-1]]\n",
    "        \n",
    "    # 추출한 token중에 문자열만 선택.\n",
    "    for token in temp:\n",
    "        morphs.append((token[0], token[1]))\n",
    "    \n",
    "    return morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7xIAQHyhSHts",
    "outputId": "5d0d99a4-cee3-4056-acc0-f33c69b14afc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ['12시 땡!' '하루가 또 가네요.']\n",
      "Preprocessed: ('<start> 시 땡 ! <end>', '<start> 하루 가 또 가 네요 . <end>')\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
    "  # s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "  s = re.sub(r\"([?.!¡,¿])\", r\" \\1 \", s) # Add spaces around punctuations\n",
    "  s = re.sub(r'[\" \"]+', \" \", s) # Remove extra space\n",
    "  s = re.sub(r\"[^a-zA-Z?.!ㄱ-ㅎㅏ-ㅣ가-힣]+\", \" \", s) # Remove other characters\n",
    "  s = \" \".join(mecab_morphs(s))\n",
    "  s = s.strip()\n",
    "  s = '<start> ' + s + ' <end>'\n",
    "  return s\n",
    "\n",
    "print(\"Original:\", lines[0])\n",
    "sentences = [(preprocess(en), preprocess(es)) for (en, es) in lines]\n",
    "print(\"Preprocessed:\", sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "g3bpAfYoSdTu",
    "outputId": "ec9117a6-dd42-4b80-a71f-c6c48fe9d952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence: [1, 55, 6302, 89, 2]\n",
      "Padded: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    1   55 6302   89    2]\n"
     ]
    }
   ],
   "source": [
    "source_sentences, target_sentences = list(zip(*sentences))\n",
    "\n",
    "# In this illustration, I choose not to specify num_words and oov_token due to the size of data.\n",
    "# for details, please visit https://keras.io/preprocessing/text/\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='') \n",
    "\n",
    "\n",
    "# 단어 사전 하나로 만들기.\n",
    "tokenizer.fit_on_texts(source_sentences + target_sentences)\n",
    "\n",
    "source_data = tokenizer.texts_to_sequences(source_sentences)\n",
    "target_data = tokenizer.texts_to_sequences(target_sentences)\n",
    "print(\"Sequence:\", source_data[0])\n",
    "\n",
    "# 입력값은 앞에 패딩주기\n",
    "source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='pre',maxlen=20)\n",
    "target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post',maxlen=20)\n",
    "print(\"Padded:\", source_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "Tnlm3gEtSsb_",
    "outputId": "38c863af-98f3-4b79-dba6-b255fca60edf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sequence [  1 205   8 173   8  64   3   2   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0]\n",
      "Target label [205.   8. 173.   8.  64.   3.   2.   0.   0.   0.   0.   0.   0.   0.\n",
      "   0.   0.   0.   0.   0.   0.]\n",
      "Size of vocabulary:  9113\n"
     ]
    }
   ],
   "source": [
    "# Machine translation models take the entire source sentence and an incomplete sentence in\n",
    "# target language as inputs at once, and predict the next word for the incomplete sentence.\n",
    "# We create labels for the decoder by shifting the target sequence one to the right.\n",
    "target_labels = np.zeros(target_data.shape)\n",
    "target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
    "\n",
    "print(\"Target sequence\", target_data[0])\n",
    "print(\"Target label\", target_labels[0])\n",
    "\n",
    "vocab_len = len(tokenizer.word_index) + 1\n",
    "# source_vocab_len = len(source_tokenizer.word_index) + 1\n",
    "# target_vocab_len = len(target_tokenizer.word_index) + 1\n",
    "\n",
    "print(\"Size of vocabulary: \", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359672, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## word2vec \n",
    "import gensim\n",
    "ko_model = gensim.models.Word2Vec.load('./word2vec_model_200')\n",
    "ko_model.wv.vectors.shape #30만 단어 200차원으로 학습한 모델링 가지고 오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9113, 200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_len, 200))\n",
    "# 단어 집합 크기의 행과 200개의 열을 가지는 행렬 생성. 값은 전부 0으로 채워진다.\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in ko_model:\n",
    "        return ko_model[word]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\.conda\\envs\\team2_real\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \n",
      "C:\\Users\\USER\\.conda\\envs\\team2_real\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    val = tokenizer.index_word[idx]\n",
    "    temp = get_vector(val) # 단어(key) 해당되는 임베딩 벡터의 200개의 값(value)를 임시 변수에 저장\n",
    "    if temp is not None: # 만약 None이 아니라면 임베딩 벡터의 값을 리턴받은 것이므로\n",
    "        embedding_matrix[idx] = temp # 해당 단어 위치의 행에 벡터의 값을 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0-oKCaIAWOSX"
   },
   "outputs": [],
   "source": [
    "# For Gradient Tape training\n",
    "dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(32)\n",
    "# For Keras model.fit()\n",
    "dataset_2 = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrfG2a42TB7F"
   },
   "source": [
    "# Transformer 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ejOHRGKg-1Oh"
   },
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "# 컴퓨터 사정상 작은 파라미터를 사용하였습니다\n",
    "\n",
    "# d_model = 64 # 512 in the original paper\n",
    "d_model = 200 # 워드 임베딩 200으로 함.\n",
    "d_k = 16 # 64 in the original paper\n",
    "d_v = 16 # 64 in the original paper\n",
    "n_heads = 4 # 8 in the original paper\n",
    "n_encoder_layers = 2 # 6 in the original paper\n",
    "n_decoder_layers = 2 # 6 in the original paper\n",
    "\n",
    "max_token_length = 20 # 512 in the original paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nqXGnAVw_ow3"
   },
   "outputs": [],
   "source": [
    "class SingleHeadAttention(Layer):\n",
    "  def __init__(self, input_shape=(3, -1, d_model), dropout=.0, masked=None):\n",
    "    super(SingleHeadAttention, self).__init__()\n",
    "    self.q = Dense(d_k, input_shape=(-1, d_model), kernel_initializer='glorot_uniform', \n",
    "                   bias_initializer='glorot_uniform')\n",
    "    self.normalize_q = Lambda(lambda x: x / np.sqrt(d_k))\n",
    "    self.k = Dense(d_k, input_shape=(-1, d_model), kernel_initializer='glorot_uniform', \n",
    "                   bias_initializer='glorot_uniform')\n",
    "    self.v = Dense(d_v, input_shape=(-1, d_model), kernel_initializer='glorot_uniform', \n",
    "                   bias_initializer='glorot_uniform')\n",
    "    self.dropout = dropout\n",
    "    self.masked = masked\n",
    "  \n",
    "  # Inputs: [query, key, value]\n",
    "  def call(self, inputs, training=None):\n",
    "    assert len(inputs) == 3\n",
    "    # We use a lambda layer to divide vector q by sqrt(d_k) according to the equation\n",
    "    q = self.normalize_q(self.q(inputs[0]))\n",
    "    k = self.k(inputs[1])\n",
    "    # The dimensionality of q is (batch_size, query_length, d_k) and that of k is (batch_size, key_length, d_k)\n",
    "    # So we will do a matrix multication by batch after transposing last 2 dimensions of k\n",
    "    # tf.shape(attn_weights) = (batch_size, query_length, key_length)\n",
    "    attn_weights = tf.matmul(q, tf.transpose(k, perm=[0,2,1]))\n",
    "    if self.masked: # Prevent future attentions in decoding self-attention\n",
    "      # Create a matrix where the strict upper triangle (not including main diagonal) is filled with -inf and 0 elsewhere\n",
    "      length = tf.shape(attn_weights)[-1]\n",
    "      #attn_mask = np.triu(tf.fill((length, length), -np.inf), k=1) # We need to use tensorflow functions instead of numpy\n",
    "      attn_mask = tf.fill((length, length), -np.inf)\n",
    "      attn_mask = tf.linalg.band_part(attn_mask, 0, -1) # Get upper triangle\n",
    "      attn_mask = tf.linalg.set_diag(attn_mask, tf.zeros((length))) # Set diagonal to zeros to avoid operations with infinity\n",
    "      # This matrix is added to the attention weights so all future attention will have -inf logits (0 after softmax)\n",
    "      attn_weights += attn_mask\n",
    "    # Softmax along the last dimension\n",
    "    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "    if training: # Attention dropout included in the original paper. This is possibly to encourage multihead diversity.\n",
    "      attn_weights = tf.nn.dropout(attn_weights, rate=self.dropout)\n",
    "    v = self.v(inputs[2])\n",
    "    return tf.matmul(attn_weights, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DiolVDYwFk00"
   },
   "outputs": [],
   "source": [
    "# 멀티 헤드 어텐션\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "  def __init__(self, dropout=.0, masked=None):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.attn_heads = list()\n",
    "    for i in range(n_heads): \n",
    "      self.attn_heads.append(SingleHeadAttention(dropout=dropout, masked=masked))\n",
    "    self.linear = Dense(d_model, input_shape=(-1, n_heads * d_v), kernel_initializer='glorot_uniform', \n",
    "                   bias_initializer='glorot_uniform')\n",
    "    \n",
    "  def call(self, x, training=None):\n",
    "    attentions = [self.attn_heads[i](x, training=training) for i in range(n_heads)]\n",
    "    concatenated_attentions = tf.concat(attentions, axis=-1)\n",
    "    return self.linear(concatenated_attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gIyEIPm2sq9O"
   },
   "outputs": [],
   "source": [
    "# 인코더 \n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "  def __init__(self, dropout=.1, attention_dropout=.0, **kwargs):\n",
    "    super(TransformerEncoder, self).__init__(**kwargs)\n",
    "    self.dropout_rate = dropout\n",
    "    self.attention_dropout_rate = attention_dropout\n",
    "  def build(self, input_shape):\n",
    "    self.multihead_attention = MultiHeadAttention(dropout=self.attention_dropout_rate)\n",
    "    self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    self.layer_normalization1 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "    \n",
    "    self.linear1 = Dense(input_shape[-1] * 4, input_shape=input_shape, activation='relu',\n",
    "                        kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    self.linear2 = Dense(input_shape[-1], input_shape=self.linear1.compute_output_shape(input_shape),\n",
    "                        kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    self.layer_normalization2 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "    super(TransformerEncoder, self).build(input_shape)\n",
    "  def call(self, x, training=None):\n",
    "    sublayer1 = self.multihead_attention((x, x, x), training=training)\n",
    "    sublayer1 = self.dropout1(sublayer1, training=training)\n",
    "    layernorm1 = self.layer_normalization1(x + sublayer1)\n",
    "    \n",
    "    sublayer2 = self.linear2(self.linear1(layernorm1))\n",
    "    sublayer1 = self.dropout2(sublayer2, training=training)\n",
    "    layernorm2 = self.layer_normalization2(layernorm1 + sublayer2)\n",
    "    return layernorm2\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u6FC8ZLy8_-R"
   },
   "outputs": [],
   "source": [
    "# 디코더\n",
    "\n",
    "class TransformerDecoder(Layer):\n",
    "  def __init__(self, dropout=.0, attention_dropout=.0, **kwargs):\n",
    "    super(TransformerDecoder, self).__init__(**kwargs)\n",
    "    self.dropout_rate = dropout\n",
    "    self.attention_dropout_rate = attention_dropout\n",
    "  def build(self, input_shape):\n",
    "    self.multihead_self_attention = MultiHeadAttention(dropout=self.attention_dropout_rate, masked=True)\n",
    "    self.dropout1 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    self.layer_normalization1 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "    \n",
    "    self.multihead_encoder_attention = MultiHeadAttention(dropout=self.attention_dropout_rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    self.layer_normalization2 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "    \n",
    "    self.linear1 = Dense(input_shape[-1] * 4, input_shape=input_shape, activation='relu',\n",
    "                        kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    self.linear2 = Dense(input_shape[-1], input_shape=self.linear1.compute_output_shape(input_shape),\n",
    "                        kernel_initializer='glorot_uniform', bias_initializer='glorot_uniform')\n",
    "    self.dropout3 = tf.keras.layers.Dropout(self.dropout_rate)\n",
    "    self.layer_normalization3 = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "    super(TransformerDecoder, self).build(input_shape)\n",
    "  def call(self, x, hidden, training=None):\n",
    "    sublayer1 = self.multihead_self_attention((x, x, x))\n",
    "    sublayer1 = self.dropout1(sublayer1, training=training)\n",
    "    layernorm1 = self.layer_normalization1(x + sublayer1)\n",
    "    \n",
    "    sublayer2 = self.multihead_encoder_attention((x, hidden, hidden))\n",
    "    sublayer2 = self.dropout2(sublayer2, training=training)\n",
    "    layernorm2 = self.layer_normalization2(layernorm1 + sublayer2)\n",
    "    \n",
    "    sublayer3 = self.linear2(self.linear1(layernorm1))\n",
    "    sublayer3 = self.dropout3(sublayer3, training=training)\n",
    "    layernorm3 = self.layer_normalization2(layernorm2 + sublayer3)\n",
    "    return layernorm3\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "EAuiM6DUNT3p",
    "outputId": "9d2eb968-fd2f-4221-c0bf-f124f08bd7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3 26 16], shape=(3,), dtype=int32)\n",
      "tf.Tensor([  3  18 200], shape=(3,), dtype=int32)\n",
      "tf.Tensor([  3  26 200], shape=(3,), dtype=int32)\n",
      "tf.Tensor([  3  26 200], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    # Testing if the dimension matches!\n",
    "    x = tf.ones((3, 26, d_model))\n",
    "    x1 = tf.ones((3, 18, d_model))\n",
    "    single_att = SingleHeadAttention(masked=None)\n",
    "    multi_att = MultiHeadAttention()\n",
    "    encoder = TransformerEncoder()\n",
    "    decoder = TransformerDecoder()\n",
    "    y = single_att((x, x, x)) # Self attention\n",
    "    y1 = multi_att((x1, x, x)) # Encoder-decoder attention\n",
    "    print(tf.shape(y))\n",
    "    print(tf.shape(y1))\n",
    "    y2 = encoder(x)\n",
    "    y3 = decoder(x, y2)\n",
    "\n",
    "    print(tf.shape(y2))\n",
    "    print(tf.shape(y3))\n",
    "    #print(layer.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-5ZOk2dowenm"
   },
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(Layer): #Inspired from https://github.com/graykode/nlp-tutorial/blob/master/5-1.Transformer/Transformer_Torch.ipynb\n",
    "  def __init__(self):\n",
    "    super(SinusoidalPositionalEncoding, self).__init__()\n",
    "    self.sinusoidal_encoding = np.array([self.get_positional_angle(pos) for pos in range(max_token_length)], dtype=np.float32)\n",
    "    self.sinusoidal_encoding[:, 0::2] = np.sin(self.sinusoidal_encoding[:, 0::2])\n",
    "    self.sinusoidal_encoding[:, 1::2] = np.cos(self.sinusoidal_encoding[:, 1::2])\n",
    "    self.sinusoidal_encoding = tf.cast(self.sinusoidal_encoding, dtype=tf.float32) # Casting the array to Tensor for slicing\n",
    "  def call(self, x):\n",
    "    return x + self.sinusoidal_encoding[:tf.shape(x)[1]]\n",
    "    #return x + tf.slice(self.sinusoidal_encoding, [0, 0], [tf.shape(x)[1], d_model])\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    return input_shape\n",
    "  def get_angle(self, pos, dim):\n",
    "    return pos / np.power(10000, 2 * (dim // 2) / d_model)\n",
    "  def get_positional_angle(self, pos):\n",
    "    return [self.get_angle(pos, dim) for dim in range(d_model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wlhp-s1G60FQ"
   },
   "source": [
    "## Assembling the Full Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OPyXChj3Xjet"
   },
   "outputs": [],
   "source": [
    "class Transformer(Model):\n",
    "  def __init__(self, dropout=.1, attention_dropout=.0, **kwargs):\n",
    "    super(Transformer, self).__init__(**kwargs)\n",
    "    self.encoding_embedding = Embedding(vocab_len, d_model, weights=[embedding_matrix], name='en-embedding')\n",
    "    self.decoding_embedding = Embedding(vocab_len, d_model, weights=[embedding_matrix], name='de-embedding')\n",
    "    self.pos_encoding = SinusoidalPositionalEncoding()\n",
    "    self.encoder = [TransformerEncoder(dropout=dropout, attention_dropout=attention_dropout) for i in range(n_encoder_layers)]\n",
    "    self.decoder = [TransformerDecoder(dropout=dropout, attention_dropout=attention_dropout) for i in range(n_decoder_layers)]\n",
    "    self.decoder_final = Dense(vocab_len, input_shape=(None, d_model))\n",
    "  def call(self, inputs, training=None): # Source_sentence and decoder_input\n",
    "    source_sentence, decoder_input = inputs\n",
    "    embedded_source = self.encoding_embedding(source_sentence)\n",
    "    encoder_output = self.pos_encoding(embedded_source)\n",
    "    for encoder_unit in self.encoder:\n",
    "      encoder_output = encoder_unit(encoder_output, training=training)\n",
    "    \n",
    "    embedded_target = self.decoding_embedding(decoder_input)\n",
    "    decoder_output = self.pos_encoding(embedded_target)\n",
    "    for decoder_unit in self.decoder:\n",
    "      decoder_output = decoder_unit(decoder_output, encoder_output, training=training)\n",
    "    if training:\n",
    "      decoder_output = self.decoder_final(decoder_output)\n",
    "      decoder_output = tf.nn.softmax(decoder_output, axis=-1)\n",
    "    else:\n",
    "      decoder_output = self.decoder_final(decoder_output[:, -1:, :])\n",
    "      decoder_output = tf.nn.softmax(decoder_output, axis=-1)\n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wN3g869inv7e"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TQS00n7DfcYZ",
    "outputId": "34bc9c62-905f-40fd-d58c-7bbdd031f8a3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000244A1FF98B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000244A1FF98B8> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Bad argument number for Name: 4, expecting 3\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "907/907 [==============================] - 53s 59ms/step - loss: 1.7624 - accuracy: 0.6938\n",
      "Epoch 2/50\n",
      "907/907 [==============================] - 53s 59ms/step - loss: 0.9231 - accuracy: 0.8177\n",
      "Epoch 3/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.7452 - accuracy: 0.8468\n",
      "Epoch 4/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.6615 - accuracy: 0.8605\n",
      "Epoch 5/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.6070 - accuracy: 0.8690\n",
      "Epoch 6/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.5723 - accuracy: 0.8745\n",
      "Epoch 7/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.5427 - accuracy: 0.8795\n",
      "Epoch 8/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.5190 - accuracy: 0.8836\n",
      "Epoch 9/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.4989 - accuracy: 0.8872\n",
      "Epoch 10/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.4817 - accuracy: 0.8900\n",
      "Epoch 11/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.4652 - accuracy: 0.89290s - loss: 0.4652 - accuracy: 0.89\n",
      "Epoch 12/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.4516 - accuracy: 0.8954\n",
      "Epoch 13/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.4398 - accuracy: 0.8973\n",
      "Epoch 14/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.4282 - accuracy: 0.8995\n",
      "Epoch 15/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.4168 - accuracy: 0.9014\n",
      "Epoch 16/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.4066 - accuracy: 0.9032\n",
      "Epoch 17/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3967 - accuracy: 0.9050\n",
      "Epoch 18/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3882 - accuracy: 0.9067\n",
      "Epoch 19/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3792 - accuracy: 0.9082\n",
      "Epoch 20/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3733 - accuracy: 0.9090\n",
      "Epoch 21/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3637 - accuracy: 0.9112\n",
      "Epoch 22/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3582 - accuracy: 0.9121\n",
      "Epoch 23/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3479 - accuracy: 0.9139\n",
      "Epoch 24/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3426 - accuracy: 0.9148\n",
      "Epoch 25/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3368 - accuracy: 0.9158\n",
      "Epoch 26/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3307 - accuracy: 0.9170\n",
      "Epoch 27/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.3238 - accuracy: 0.9183\n",
      "Epoch 28/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3188 - accuracy: 0.9190\n",
      "Epoch 29/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3109 - accuracy: 0.9211\n",
      "Epoch 30/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.3079 - accuracy: 0.9212\n",
      "Epoch 31/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.3012 - accuracy: 0.9226\n",
      "Epoch 32/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2964 - accuracy: 0.9235\n",
      "Epoch 33/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2897 - accuracy: 0.9248\n",
      "Epoch 34/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2851 - accuracy: 0.92560s - l\n",
      "Epoch 35/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2848 - accuracy: 0.9257\n",
      "Epoch 36/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2765 - accuracy: 0.9277\n",
      "Epoch 37/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2715 - accuracy: 0.9286\n",
      "Epoch 38/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2679 - accuracy: 0.9292\n",
      "Epoch 39/50\n",
      "907/907 [==============================] - 52s 58ms/step - loss: 0.2620 - accuracy: 0.93040s - loss: 0.261\n",
      "Epoch 40/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.2568 - accuracy: 0.9316\n",
      "Epoch 41/50\n",
      "907/907 [==============================] - 55s 60ms/step - loss: 0.2539 - accuracy: 0.9320\n",
      "Epoch 42/50\n",
      "907/907 [==============================] - 51s 57ms/step - loss: 0.2490 - accuracy: 0.9332\n",
      "Epoch 43/50\n",
      "907/907 [==============================] - 51s 56ms/step - loss: 0.2460 - accuracy: 0.9338\n",
      "Epoch 44/50\n",
      "907/907 [==============================] - 51s 56ms/step - loss: 0.2426 - accuracy: 0.9342\n",
      "Epoch 45/50\n",
      "907/907 [==============================] - 51s 56ms/step - loss: 0.2368 - accuracy: 0.9357\n",
      "Epoch 46/50\n",
      "907/907 [==============================] - 51s 56ms/step - loss: 0.2339 - accuracy: 0.9360\n",
      "Epoch 47/50\n",
      "907/907 [==============================] - 51s 56ms/step - loss: 0.2319 - accuracy: 0.9361\n",
      "Epoch 48/50\n",
      "907/907 [==============================] - 51s 56ms/step - loss: 0.2268 - accuracy: 0.9379\n",
      "Epoch 49/50\n",
      "907/907 [==============================] - 53s 58ms/step - loss: 0.2231 - accuracy: 0.9382\n",
      "Epoch 50/50\n",
      "907/907 [==============================] - 53s 59ms/step - loss: 0.2208 - accuracy: 0.9389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x244a2252b48>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_2 = Transformer() # Instantiating a new transformer model\n",
    "\n",
    "src_seqs, tgt_seqs, tgt_labels = zip(*dataset_2)\n",
    "train = [tf.cast(src_seqs, dtype=tf.float32), tf.cast(tgt_seqs, dtype=tf.float32)] # Cast the tuples to tensors\n",
    "\n",
    "transformer_2.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "transformer_2.fit(train, tf.cast(tgt_labels, dtype=tf.float32), verbose=1, batch_size=32, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Func(dropout,attention_dropout):\n",
    "    encoding_embedding = Embedding(vocab_len, d_model)\n",
    "    decoding_embedding = Embedding(vocab_len, d_model)\n",
    "    pos_encoding = SinusoidalPositionalEncoding()\n",
    "    encoder = [TransformerEncoder(dropout=dropout, attention_dropout=attention_dropout) for i in range(n_encoder_layers)]\n",
    "    decoder = [TransformerDecoder(dropout=dropout, attention_dropout=attention_dropout) for i in range(n_decoder_layers)]\n",
    "    decoder_final = Dense(vocab_len, input_shape=(None, d_model))\n",
    "    \n",
    "    training = False\n",
    "    for batch, (source_sentence, decoder_input, target_labels) in enumerate(dataset):\n",
    "#     source_sentence, decoder_input = inputs\n",
    "        embedded_source = encoding_embedding(source_sentence)\n",
    "        encoder_output = pos_encoding(embedded_source)\n",
    "    \n",
    "        for encoder_unit in encoder:\n",
    "          encoder_output = encoder_unit(encoder_output, training=training)\n",
    "    \n",
    "        embedded_target = decoding_embedding(decoder_input)\n",
    "        decoder_output = pos_encoding(embedded_target)\n",
    "        for decoder_unit in decoder:\n",
    "          decoder_output = decoder_unit(decoder_output, encoder_output, training=training)\n",
    "        if training:\n",
    "          decoder_output = decoder_final(decoder_output)\n",
    "          decoder_output = tf.nn.softmax(decoder_output, axis=-1)\n",
    "        else:\n",
    "          decoder_output = decoder_final(decoder_output[:, -1:, :])\n",
    "          decoder_output = tf.nn.softmax(decoder_output, axis=-1)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load_weights('transformer_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_3 = Transformer_Func(dropout=.1, attention_dropout=.0)\n",
    "# transformer_3.load_weights('transformer_weight.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2cLbLI5Shg0P",
    "outputId": "b6855013-d97f-48c5-eaf6-2f0ef8305da5"
   },
   "outputs": [],
   "source": [
    "%cd gdrive/'My Drive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UcZ9Erkn9Z9h"
   },
   "source": [
    "## 모델 예측 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tb6DEeIuLSOt"
   },
   "outputs": [],
   "source": [
    "def translate(model, source_sentence, target_sentence_start=[['<start>']]):\n",
    "  if np.ndim(source_sentence) == 1: # Create a batch of 1 the input is a sentence\n",
    "    source_sentence = [source_sentence]\n",
    "  if np.ndim(target_sentence_start) == 1:\n",
    "    target_sentence_start = [target_sentence_start]\n",
    "  # Tokenizing and padding\n",
    "  source_seq = tokenizer.texts_to_sequences(source_sentence)\n",
    "  source_seq = tf.keras.preprocessing.sequence.pad_sequences(source_seq, padding='pre', maxlen=15)\n",
    "  predict_seq = tokenizer.texts_to_sequences(target_sentence_start)\n",
    "  \n",
    "  predict_sentence = list(target_sentence_start[0]) # Deep copy here to prevent updates on target_sentence_start\n",
    "  while predict_sentence[-1] != '<end>' and len(predict_seq) < max_token_length:\n",
    "    predict_output = model([np.array(source_seq), np.array(predict_seq)], training=None)\n",
    "    predict_label = tf.argmax(predict_output, axis=-1) # Pick the label with highest softmax score\n",
    "    predict_seq = tf.concat([predict_seq, predict_label], axis=-1) # Updating the prediction sequence\n",
    "    predict_sentence.append(tokenizer.index_word[predict_label[0][0].numpy()])\n",
    "  \n",
    "  return predict_sentence[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "colab_type": "code",
    "id": "TUJVgmDhRQLm",
    "outputId": "4eb95505-9f8b-4b15-9b92-2998756ad830"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source sentence:  <start> 시 땡 ! <end>\n",
      "Target sentence:  <start> 하루 가 또 가 네요 . <end>\n",
      "Predicted sentence:  <start> 달달 하루 노래 . <end>\n",
      "\n",
      "Source sentence:  <start> SNS 보 면 나 만 빼 고 다 행복 해 보여 <end>\n",
      "Target sentence:  <start> 자랑 하 는 자리 니까요 . <end>\n",
      "Predicted sentence:  <start> 자랑 하 는 자리 니까요 . <end>\n",
      "\n",
      "Source sentence:  <start> 가스 비 비싼데 감기 걸리 겠 어 <end>\n",
      "Target sentence:  <start> 따뜻 하 게 사세요 ! <end>\n",
      "Predicted sentence:  <start> 이럴 때 잘 쉬 는 게 중요 해요 . <end>\n",
      "\n",
      "Source sentence:  <start> 가족 들 이랑 서먹 해 <end>\n",
      "Target sentence:  <start> 다 들 바빠서 이야기 할 시간 이 부족 했 나 봐요 . <end>\n",
      "Predicted sentence:  <start> 완벽 한 사람 은 아무 도 없 어요 . <end>\n",
      "\n",
      "Source sentence:  <start> 간만에 떨리 니까 좋 더라 <end>\n",
      "Target sentence:  <start> 떨리 는 감정 은 그 자체 로 소중 해요 . <end>\n",
      "Predicted sentence:  <start> 애석 하 지만 당신 을 더 소중히 생각 해 주 세요 . <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "  print(\"Source sentence: \", source_sentences[i*10])\n",
    "  print(\"Target sentence: \", target_sentences[i*10])\n",
    "  print(\"Predicted sentence: \", ' '.join(translate(transformer_2, source_sentences[i*10].split(' '))))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "  print(\"Source sentence: \", source_sentences[20500+i*5])\n",
    "  print(\"Target sentence: \", target_sentences[20500+i*5])\n",
    "  print(\"Predicted sentence: \", ' '.join(translate(transformer_3, source_sentences[20500+i*5].split(' '))))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5J13BiKrnZtk",
    "outputId": "66844545-a0bd-47b1-8a2c-9ac306f7862d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentence:  <start> 맛있 게 드세요 . <end>\n"
     ]
    }
   ],
   "source": [
    "  print(\"Predicted sentence: \", ' '.join(translate(transformer_2, '<start> 밥 뭐 먹지? <end>'.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MTf0hvmuNgV"
   },
   "source": [
    "# 모델 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "mBwqqSvnuLx2",
    "outputId": "d10116fd-349d-48d9-bf82-94d759284b1b"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# 딕셔너리 저장\n",
    "# tokenizer.word_index, tokenizer.index_word >> 케라스에서 생성해준 딕셔너리를 볼 수 있음.\n",
    "file = open(\"word_to_index\",\"wb\") \n",
    "pickle.dump(tokenizer.word_index,file) \n",
    "file.close()\n",
    "file = open(\"index_to_word\",\"wb\") \n",
    "pickle.dump(tokenizer.index_word,file) \n",
    "file.close()\n",
    "\n",
    "# 모델 weight저장\n",
    "transformer.save_weights(\"transformer_weight.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트셋 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미유야 안녕</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>반가워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>넌 누구니?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>고마워</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>고맙다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Q\n",
       "0  미유야 안녕\n",
       "1     반가워\n",
       "2  넌 누구니?\n",
       "3     고마워\n",
       "4     고맙다"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tests= pd.read_csv('test.csv',names=['Q'])\n",
    "tests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['미유야 안녕', '반가워', '넌 누구니?', '고마워', '고맙다', '심심해', '점심 뭐 먹지?',\n",
       "       '오늘 김팀장님한테 혼나서 짜증났어', '사람들 모두 날 싫어 하는거 같아', '오늘 기분이 별로 안좋아',\n",
       "       '요즘 생각이 많아지는 것 같아', '아.. 운동가야하는데 너무 귀찮아..', '그냥 여행이나 가고 싶다',\n",
       "       '좋아하는 사람이 날 안좋아하는 거 같아', '여자친구랑 싸웠어..', '과장님때문에 힘들다',\n",
       "       '부모님을 이해할 수 없어', '취업하기 너무 어렵다', '나를 원하는 사람이 없어', '미세먼지 너무 많아',\n",
       "       '오늘 몸이 안좋네', '인생이 재미 없다', '오늘 컨디션이 안좋아', '내일은 나아질까?', '내일은 좋아질까?',\n",
       "       '요새 스트레스가 너무 심해', '재밌는거 뭐 없나?', '부모님이 보고싶어', '오늘 친구랑 싸웠어',\n",
       "       '친구들이랑 사이가 안좋아', '아빠가 날 힘들게해', '오늘 친구랑 싸웠어', '오늘 할 일이 너무 많다',\n",
       "       '여자친구랑 헤어졌어', '아...회사가기 싫다', '오늘 학교가기 싫어', '아 내일도 회사 가야해',\n",
       "       '일 그만 둘까?', '오늘 뭐 하지?', '남자친구 때문에 짜증나', '우울하다', '오늘 하루 힘들었다',\n",
       "       '집에 혼자있으니까 적적하다', '엄마는 날 이해하지 못하는 것 같아', '아침부터 피곤하다', '오늘 뭐 했어?',\n",
       "       '넌 잘하는게 뭐야?', '나 힘들어', '진짜 싫다', '김부장 재수없어', '짜증난다'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X= tests['Q'].values\n",
    "test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index로 변경 후 pad줌.\n",
    "x = [(preprocess(x)) for x in test_X]\n",
    "x = tokenizer.texts_to_sequences(x)\n",
    "x = tf.keras.preprocessing.sequence.pad_sequences(x, padding='pre',maxlen=max_token_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   86,  852,    2],\n",
       "       [   0,    0,    0, ...,    1, 6472,    2],\n",
       "       [   0,    0,    0, ...,  178,   12,    2],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  160,   40,    2],\n",
       "       [   0,    0,    0, ...,    1,   15,    2],\n",
       "       [   0,    0,    0, ...,  310, 3509,    2]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test셋 예측값을 (단어 임베딩된 상태의) 리스트로 출력\n",
    "def predict_text(x_set):\n",
    "    y_pred = []\n",
    "    # 전처리\n",
    "    x_set = [(preprocess(x)) for x in x_set]\n",
    "    # 예측하기\n",
    "    for x in x_set:\n",
    "        pred_seq = ' '.join(translate(transformer_2, x.split(' '))) # 덱스트형태 리스트\n",
    "        y_pred.append(pred_seq)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_text(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests['A']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>미유야 안녕</td>\n",
       "      <td>안녕 하 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>반가워</td>\n",
       "      <td>반갑 안녕 하 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>넌 누구니?</td>\n",
       "      <td>저 는 위 로 봇 입니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>고마워</td>\n",
       "      <td>감사 해요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>고맙다</td>\n",
       "      <td>감사 해요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>심심해</td>\n",
       "      <td>노래 불러 드릴까요 ? 북치 기박 치 기 헥헥</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>점심 뭐 먹지?</td>\n",
       "      <td>오늘 은 짜장면 어떠세요 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>오늘 김팀장님한테 혼나서 짜증났어</td>\n",
       "      <td>마음 이 헛헛 한가 봐요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>사람들 모두 날 싫어 하는거 같아</td>\n",
       "      <td>그걸 깨닫 다니 대단 하 시 군요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>오늘 기분이 별로 안좋아</td>\n",
       "      <td>정도 가 심하 면 병원 을 찾 아 보 는 것 도 좋 을 것 같 아요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>요즘 생각이 많아지는 것 같아</td>\n",
       "      <td>지금 도 충분히 아름다워요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>아.. 운동가야하는데 너무 귀찮아..</td>\n",
       "      <td>가벼운 운동 을 해 보 는 건 어떠세요 ? 산책 도 좋 답니다 !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>그냥 여행이나 가고 싶다</td>\n",
       "      <td>마음 정리 하 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>좋아하는 사람이 날 안좋아하는 거 같아</td>\n",
       "      <td>그렇게 생각 하 는 게 도움 이 될 때 만큼 크 게 넣 어 주 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>여자친구랑 싸웠어..</td>\n",
       "      <td>친구 가 많 은 남자 친구 가 있 는 건 아니 랍니다 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>과장님때문에 힘들다</td>\n",
       "      <td>잘 버티 고 있 어요 . 조금 힘들 더라도 조금 만 기운 을 내 요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>부모님을 이해할 수 없어</td>\n",
       "      <td>싸우 면서 크 는 거 죠 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>취업하기 너무 어렵다</td>\n",
       "      <td>바늘 구멍 이 니까요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>나를 원하는 사람이 없어</td>\n",
       "      <td>아무 것 도 아니 라고 생각 하 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>미세먼지 너무 많아</td>\n",
       "      <td>살 기 더 힘들 어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>오늘 몸이 안좋네</td>\n",
       "      <td>일 이 반복 되 면 정말 힘든 것 같 아요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>인생이 재미 없다</td>\n",
       "      <td>새벽 이 가장 어둡 다고 하 잖아요 . 곧 동 이 틀 거 라고 생각 해요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>오늘 컨디션이 안좋아</td>\n",
       "      <td>너무 바쁘 신 것 같 아요 . 잠시 숨 을 돌려보 는 건 어떨까 요 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>내일은 나아질까?</td>\n",
       "      <td>그런 기분 이 들 면 정말 이상 할 것 같 아요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>내일은 좋아질까?</td>\n",
       "      <td>새벽 이 가장 어둡 다고 하 잖아요 . 곧 동 이 틀 거 라고 생각 해요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>요새 스트레스가 너무 심해</td>\n",
       "      <td>잘 푸 는 것 도 중요 해요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>재밌는거 뭐 없나?</td>\n",
       "      <td>잼 있 는 거 는 딸기 잼 에게 물어봐 주 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>부모님이 보고싶어</td>\n",
       "      <td>엄마 께 프라이버시 를 지켜 달 라고 말 해 보 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>오늘 친구랑 싸웠어</td>\n",
       "      <td>친구 를 사귀 어 보 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>친구들이랑 사이가 안좋아</td>\n",
       "      <td>슬픈 이야기 네요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>아빠가 날 힘들게해</td>\n",
       "      <td>잘 버티 고 있 어요 . 조금 힘들 더라도 조금 만 기운 을 내 요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>오늘 친구랑 싸웠어</td>\n",
       "      <td>친구 를 사귀 어 보 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>오늘 할 일이 너무 많다</td>\n",
       "      <td>조금 씩 더 좋 은 곳 으로 옮길 수 있 을 거 예요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>여자친구랑 헤어졌어</td>\n",
       "      <td>만나 지 마세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>아...회사가기 싫다</td>\n",
       "      <td>일 하 는 건 어떠신가요 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>오늘 학교가기 싫어</td>\n",
       "      <td>학교 를 안 가 게 된 이유 가 있 으신 가요 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>아 내일도 회사 가야해</td>\n",
       "      <td>일 이 반복 되 면 정말 힘든 것 같 아요 . 기운 내 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>일 그만 둘까?</td>\n",
       "      <td>저 에게 는 늘 당신 이 우선 인데 마음 이 아파요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>오늘 뭐 하지?</td>\n",
       "      <td>화 를 쏟 아 내 고 나 면 기분 이 조금 풀릴 거 예요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>남자친구 때문에 짜증나</td>\n",
       "      <td>진심 으로 격려 하 고 싶 어 주 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>우울하다</td>\n",
       "      <td>비난 과 자책 을 멈추 세요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>오늘 하루 힘들었다</td>\n",
       "      <td>그런 일 이 있 으셨 군요 . 조금 씩 나아질 거 라고 믿 어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>집에 혼자있으니까 적적하다</td>\n",
       "      <td>힘내 세요 . 멀 어 진 인연 만큼 또 가까워질 새 인연 이 있 을 거 예요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>엄마는 날 이해하지 못하는 것 같아</td>\n",
       "      <td>완벽 한 이별 은 없 어요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>아침부터 피곤하다</td>\n",
       "      <td>휴식 을 취할 때 가 된 것 아닐까요 ? 잠깐 쉬 어 가요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>오늘 뭐 했어?</td>\n",
       "      <td>바쁘 시 는 건 많 지 않 아요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>넌 잘하는게 뭐야?</td>\n",
       "      <td>조금 의 용기 를 낸다면 가능 할 거 예요 .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>나 힘들어</td>\n",
       "      <td>조금 더 마음 을 달리 가져 보 는 건 어떨까 요 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>진짜 싫다</td>\n",
       "      <td>잠시 쉬 어 간다고 생각 해 보 는 건 어때요 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>김부장 재수없어</td>\n",
       "      <td>힘내 세요 !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>짜증난다</td>\n",
       "      <td>짜증 날 때 는 기분 푸 는 음악 어때요 ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Q                                             A\n",
       "0                  미유야 안녕                                     안녕 하 세요 .\n",
       "1                     반가워                                  반갑 안녕 하 세요 .\n",
       "2                  넌 누구니?                               저 는 위 로 봇 입니다 .\n",
       "3                     고마워                                       감사 해요 .\n",
       "4                     고맙다                                       감사 해요 .\n",
       "5                     심심해                     노래 불러 드릴까요 ? 북치 기박 치 기 헥헥\n",
       "6                점심 뭐 먹지?                               오늘 은 짜장면 어떠세요 ?\n",
       "7      오늘 김팀장님한테 혼나서 짜증났어                               마음 이 헛헛 한가 봐요 .\n",
       "8      사람들 모두 날 싫어 하는거 같아                          그걸 깨닫 다니 대단 하 시 군요 .\n",
       "9           오늘 기분이 별로 안좋아       정도 가 심하 면 병원 을 찾 아 보 는 것 도 좋 을 것 같 아요 .\n",
       "10       요즘 생각이 많아지는 것 같아                              지금 도 충분히 아름다워요 .\n",
       "11   아.. 운동가야하는데 너무 귀찮아..          가벼운 운동 을 해 보 는 건 어떠세요 ? 산책 도 좋 답니다 !\n",
       "12          그냥 여행이나 가고 싶다                                  마음 정리 하 세요 .\n",
       "13  좋아하는 사람이 날 안좋아하는 거 같아       그렇게 생각 하 는 게 도움 이 될 때 만큼 크 게 넣 어 주 세요 .\n",
       "14            여자친구랑 싸웠어..               친구 가 많 은 남자 친구 가 있 는 건 아니 랍니다 .\n",
       "15             과장님때문에 힘들다       잘 버티 고 있 어요 . 조금 힘들 더라도 조금 만 기운 을 내 요 .\n",
       "16          부모님을 이해할 수 없어                               싸우 면서 크 는 거 죠 .\n",
       "17            취업하기 너무 어렵다                                 바늘 구멍 이 니까요 .\n",
       "18          나를 원하는 사람이 없어                        아무 것 도 아니 라고 생각 하 세요 .\n",
       "19             미세먼지 너무 많아                                 살 기 더 힘들 어요 .\n",
       "20              오늘 몸이 안좋네                     일 이 반복 되 면 정말 힘든 것 같 아요 .\n",
       "21              인생이 재미 없다    새벽 이 가장 어둡 다고 하 잖아요 . 곧 동 이 틀 거 라고 생각 해요 .\n",
       "22            오늘 컨디션이 안좋아       너무 바쁘 신 것 같 아요 . 잠시 숨 을 돌려보 는 건 어떨까 요 ?\n",
       "23              내일은 나아질까?                  그런 기분 이 들 면 정말 이상 할 것 같 아요 .\n",
       "24              내일은 좋아질까?    새벽 이 가장 어둡 다고 하 잖아요 . 곧 동 이 틀 거 라고 생각 해요 .\n",
       "25         요새 스트레스가 너무 심해                             잘 푸 는 것 도 중요 해요 .\n",
       "26             재밌는거 뭐 없나?                  잼 있 는 거 는 딸기 잼 에게 물어봐 주 세요 .\n",
       "27              부모님이 보고싶어               엄마 께 프라이버시 를 지켜 달 라고 말 해 보 세요 .\n",
       "28             오늘 친구랑 싸웠어                              친구 를 사귀 어 보 세요 .\n",
       "29          친구들이랑 사이가 안좋아                                   슬픈 이야기 네요 .\n",
       "30             아빠가 날 힘들게해       잘 버티 고 있 어요 . 조금 힘들 더라도 조금 만 기운 을 내 요 .\n",
       "31             오늘 친구랑 싸웠어                              친구 를 사귀 어 보 세요 .\n",
       "32          오늘 할 일이 너무 많다               조금 씩 더 좋 은 곳 으로 옮길 수 있 을 거 예요 .\n",
       "33             여자친구랑 헤어졌어                                    만나 지 마세요 .\n",
       "34            아...회사가기 싫다                               일 하 는 건 어떠신가요 ?\n",
       "35             오늘 학교가기 싫어                   학교 를 안 가 게 된 이유 가 있 으신 가요 ?\n",
       "36           아 내일도 회사 가야해           일 이 반복 되 면 정말 힘든 것 같 아요 . 기운 내 세요 .\n",
       "37               일 그만 둘까?                저 에게 는 늘 당신 이 우선 인데 마음 이 아파요 .\n",
       "38               오늘 뭐 하지?             화 를 쏟 아 내 고 나 면 기분 이 조금 풀릴 거 예요 .\n",
       "39           남자친구 때문에 짜증나                       진심 으로 격려 하 고 싶 어 주 세요 .\n",
       "40                   우울하다                             비난 과 자책 을 멈추 세요 .\n",
       "41             오늘 하루 힘들었다         그런 일 이 있 으셨 군요 . 조금 씩 나아질 거 라고 믿 어요 .\n",
       "42         집에 혼자있으니까 적적하다  힘내 세요 . 멀 어 진 인연 만큼 또 가까워질 새 인연 이 있 을 거 예요 .\n",
       "43    엄마는 날 이해하지 못하는 것 같아                              완벽 한 이별 은 없 어요 .\n",
       "44              아침부터 피곤하다              휴식 을 취할 때 가 된 것 아닐까요 ? 잠깐 쉬 어 가요\n",
       "45               오늘 뭐 했어?                           바쁘 시 는 건 많 지 않 아요 .\n",
       "46             넌 잘하는게 뭐야?                     조금 의 용기 를 낸다면 가능 할 거 예요 .\n",
       "47                  나 힘들어                 조금 더 마음 을 달리 가져 보 는 건 어떨까 요 ?\n",
       "48                  진짜 싫다                   잠시 쉬 어 간다고 생각 해 보 는 건 어때요 ?\n",
       "49               김부장 재수없어                                       힘내 세요 !\n",
       "50                   짜증난다                      짜증 날 때 는 기분 푸 는 음악 어때요 ?"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Easy_Transformer(chatbot).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
